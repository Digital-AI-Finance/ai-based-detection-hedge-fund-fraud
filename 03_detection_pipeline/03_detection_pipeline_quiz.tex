\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}

% Color definitions
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}
\definecolor{mlgray}{RGB}{127, 127, 127}

% Additional colors
\definecolor{lightgray}{RGB}{240, 240, 240}
\definecolor{midgray}{RGB}{180, 180, 180}

% Apply custom colors to Madrid theme
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{palette quaternary}{bg=mlpurple,fg=white}

\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{section in toc}{fg=mlpurple}
\setbeamercolor{subsection in toc}{fg=mlblue}
\setbeamercolor{title}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamercolor{block title}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{block body}{bg=mllavender4,fg=black}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}

% Clean itemize/enumerate
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[default]

% Reduce margins for more content space
\setbeamersize{text margin left=5mm,text margin right=5mm}

% Command for bottom annotation
\newcommand{\bottomnote}[1]{%
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
\footnotesize
\textbf{#1}
}

% Command for compact list spacing
\newcommand{\compactlist}{%
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}%
\setlength{\parsep}{0pt}%
}

% Notation macros
\input{../notation}

\title{Quiz: Detection Pipeline Framework}
\subtitle{Section 03 -- Digital-AI-Finance}
\author{Joerg Osterrieder}
\institute{Zurich University of Applied Sciences (ZHAW)}
\date{2025}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Question 1: Pipeline Stages}
How many stages does the detection pipeline have?

\vspace{0.5cm}
\begin{enumerate}[a)]
\item 3
\item 4
\item 5
\item 6
\end{enumerate}

\vspace{0.5cm}
\pause
\begin{block}{Answer}
\textbf{c) 5}

The detection pipeline comprises five sequential stages: data ingestion and integration, feature engineering, model selection and training, explainability and interpretation, and deployment and monitoring.
\end{block}
\bottomnote{Source: Section 3.1}
\end{frame}

\begin{frame}{Question 2: Feature Families}
How many feature families are identified?

\vspace{0.5cm}
\begin{enumerate}[a)]
\item 3
\item 4
\item 5
\item 7
\end{enumerate}

\vspace{0.5cm}
\pause
\begin{block}{Answer}
\textbf{c) 5}

Five feature families are identified: statistical features, Benford's law features, textual features from regulatory filings, network and relational features, and temporal features.
\end{block}
\bottomnote{Source: Section 3.3}
\end{frame}

\begin{frame}{Question 3: Gradient Boosting Performance}
What $F_1$ score do gradient boosting ensembles approach?

\vspace{0.5cm}
\begin{enumerate}[a)]
\item 0.72
\item 0.78
\item 0.84
\item 0.88
\end{enumerate}

\vspace{0.5cm}
\pause
\begin{block}{Answer}
\textbf{d) 0.88}

Recent work on stacking ensembles that combine XGBoost, LightGBM, and CatBoost through a meta-learner has reported $F_1$ scores approaching 0.88, the highest among individual method families.
\end{block}
\bottomnote{Source: Section 3.4}
\end{frame}

\begin{frame}{Question 4: Deep Autoencoder AUC}
What \auc{} do deep autoencoders achieve for anomaly detection?

\vspace{0.5cm}
\begin{enumerate}[a)]
\item 0.65
\item 0.72
\item 0.79
\item 0.85
\end{enumerate}

\vspace{0.5cm}
\pause
\begin{block}{Answer}
\textbf{c) 0.79}

Deep autoencoders have achieved an \auc{} of approximately 0.79 on hedge fund return data in anomaly detection frameworks, competitive with supervised approaches despite requiring no fraud labels.
\end{block}
\bottomnote{Source: Section 3.4}
\end{frame}

\begin{frame}{Question 5: SHAP Meaning}
What does SHAP stand for?

\vspace{0.5cm}
\begin{enumerate}[a)]
\item Statistical Hypothesis Analysis Protocol
\item SHapley Additive exPlanations
\item Systematic Hedge fund Assessment Program
\item Supervised Heuristic Analysis Pipeline
\end{enumerate}

\vspace{0.5cm}
\pause
\begin{block}{Answer}
\textbf{b) SHapley Additive exPlanations}

SHAP (SHapley Additive exPlanations) values provide theoretically grounded feature attribution scores that decompose a model's prediction into the contribution of each input feature.
\end{block}
\bottomnote{Source: Section 3.5}
\end{frame}

\begin{frame}{Question 6: Fusion Approaches}
Which fusion approach is better when data sources have different scales?

\vspace{0.5cm}
\begin{enumerate}[a)]
\item Early fusion
\item Late fusion
\item Both equal
\item Neither works
\end{enumerate}

\vspace{0.5cm}
\pause
\begin{block}{Answer}
\textbf{b) Late fusion}

Late fusion processes each data source through a modality-specific feature extraction pipeline and combines results afterward, preserving native structure and accommodating different scales better than early fusion.
\end{block}
\bottomnote{Source: Section 3.2}
\end{frame}

\begin{frame}{Question 7: Feedback Loops}
What are the two feedback loops in the pipeline?

\vspace{0.5cm}
\begin{enumerate}[a)]
\item Training and testing
\item Investigator feedback and drift-triggered retraining
\item Input and output validation
\item Forward and backward propagation
\end{enumerate}

\vspace{0.5cm}
\pause
\begin{block}{Answer}
\textbf{b) Investigator feedback and drift-triggered retraining}

The pipeline includes two feedback loops from deployment back to earlier stages: investigator feedback to feature engineering and drift-triggered retraining to model selection.
\end{block}
\bottomnote{Source: Section 3.1}
\end{frame}

\end{document}

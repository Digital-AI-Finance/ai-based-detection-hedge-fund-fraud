\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}

% Color definitions
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}
\definecolor{mlgray}{RGB}{127, 127, 127}

% Additional colors
\definecolor{lightgray}{RGB}{240, 240, 240}
\definecolor{midgray}{RGB}{180, 180, 180}

% Apply custom colors to Madrid theme
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{palette quaternary}{bg=mlpurple,fg=white}

\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{section in toc}{fg=mlpurple}
\setbeamercolor{subsection in toc}{fg=mlblue}
\setbeamercolor{title}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamercolor{block title}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{block body}{bg=mllavender4,fg=black}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}

% Clean itemize/enumerate
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[default]

% Reduce margins for more content space
\setbeamersize{text margin left=5mm,text margin right=5mm}

% Command for bottom annotation
\newcommand{\bottomnote}[1]{%
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
\footnotesize
\textbf{#1}
}

% Command for compact list spacing
\newcommand{\compactlist}{%
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}%
\setlength{\parsep}{0pt}%
}

% Notation macros
\input{../notation}

\title{Quiz: Feature Engineering}
\subtitle{AI-Based Detection of Hedge Fund Fraud}
\author{Joerg Osterrieder}
\institute{Zurich University of Applied Sciences (ZHAW)}
\date{2025}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Question 1}
What is the typical feature space dimensionality for fraud detection?
\begin{itemize}\compactlist
\item[a)] 10--25
\item[b)] 25--50
\item[c)] 50--200
\item[d)] 500--1000
\end{itemize}

\pause
\begin{block}{Answer}
\textbf{c)} 50--200\\[2mm]
Most fraud detection systems use 50--200 engineered features, combining return statistics (mean, volatility, skewness), Benford's law digits, autocorrelation measures, and network metrics. Higher dimensions risk overfitting.
\end{block}
\bottomnote{Appendix A1: Feature Engineering}
\end{frame}

\begin{frame}{Question 2}
What minimum return history is needed for reliable feature engineering?
\begin{itemize}\compactlist
\item[a)] 6--12 months
\item[b)] 24--36 months
\item[c)] 48--60 months
\item[d)] 72--84 months
\end{itemize}

\pause
\begin{block}{Answer}
\textbf{b)} 24--36 months\\[2mm]
Reliable statistical features require at least 24--36 months of return history. Shorter windows reduce statistical power for autocorrelation, volatility clustering, and anomaly detection. This creates the ``cold-start'' problem.
\end{block}
\bottomnote{Appendix A1: Data Requirements}
\end{frame}

\begin{frame}{Question 3}
What is Benford's law first-digit frequency for $d=1$?
\begin{itemize}\compactlist
\item[a)] $\sim$30.1\%
\item[b)] $\sim$17.6\%
\item[c)] $\sim$11.1\%
\item[d)] $\sim$25.0\%
\end{itemize}

\pause
\begin{block}{Answer}
\textbf{a)} $\sim$30.1\%\\[2mm]
Benford's law predicts that the first digit $d=1$ appears with frequency $\log_{10}(1 + 1/1) \approx 30.1\%$. Significant deviations in reported returns suggest manipulation. Fraudsters often under-report digit 1.
\end{block}
\bottomnote{Appendix A1: Benford's Law}
\end{frame}

\begin{frame}{Question 4}
Which method is used for chi-squared Benford testing?
\begin{itemize}\compactlist
\item[a)] t-test on return means
\item[b)] Kolmogorov--Smirnov test
\item[c)] Compare observed vs expected digit frequencies
\item[d)] ANOVA across periods
\end{itemize}

\pause
\begin{block}{Answer}
\textbf{c)} Compare observed vs expected digit frequencies\\[2mm]
The chi-squared test compares observed first-digit frequencies from reported returns against Benford's theoretical distribution. High $\chi^2$ values indicate anomalous digit patterns consistent with fabrication.
\end{block}
\bottomnote{Appendix A1: Statistical Testing}
\end{frame}

\begin{frame}{Question 5}
Why is PCA less common for feature reduction in fraud detection?
\begin{itemize}\compactlist
\item[a)] Too slow computationally
\item[b)] Requires too much data
\item[c)] Cannot handle categorical features
\item[d)] Loss of interpretability
\end{itemize}

\pause
\begin{block}{Answer}
\textbf{d)} Loss of interpretability\\[2mm]
Principal Component Analysis (PCA) reduces dimensionality but creates opaque linear combinations. Regulators require interpretable features (e.g., ``serial correlation'', ``Sharpe ratio'') for audit trails and legal justification.
\end{block}
\bottomnote{Appendix A1: Dimensionality Reduction}
\end{frame}

\end{document}

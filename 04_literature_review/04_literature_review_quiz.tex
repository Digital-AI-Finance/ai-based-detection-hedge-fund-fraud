\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}

% Color definitions
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}
\definecolor{mlgray}{RGB}{127, 127, 127}

% Additional colors
\definecolor{lightgray}{RGB}{240, 240, 240}
\definecolor{midgray}{RGB}{180, 180, 180}

% Apply custom colors to Madrid theme
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{palette quaternary}{bg=mlpurple,fg=white}

\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{section in toc}{fg=mlpurple}
\setbeamercolor{subsection in toc}{fg=mlblue}
\setbeamercolor{title}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamercolor{block title}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{block body}{bg=mllavender4,fg=black}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}

% Clean itemize/enumerate
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[default]

% Reduce margins for more content space
\setbeamersize{text margin left=5mm,text margin right=5mm}

% Command for bottom annotation
\newcommand{\bottomnote}[1]{%
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
\footnotesize
\textbf{#1}
}

% Command for compact list spacing
\newcommand{\compactlist}{%
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}%
\setlength{\parsep}{0pt}%
}

% Notation macros
\input{../notation}

\title{Quiz: Review of AI-Based Detection Methods}
\subtitle{Section 04 -- Digital-AI-Finance}
\author{Joerg Osterrieder}
\institute{Zurich University of Applied Sciences (ZHAW)}
\date{2025}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Question 1: Dimmock's Logistic Regression}
What AUC range do Dimmock's logistic regression models achieve?

\vspace{0.5cm}
\begin{enumerate}[a)]
\item 0.50--0.60
\item 0.65--0.70
\item 0.75--0.80
\item 0.85--0.90
\end{enumerate}

\vspace{0.5cm}
\pause
\begin{block}{Answer}
\textbf{b) 0.65--0.70}

Dimmock et al. applied logistic regression to SEC filing data, demonstrating that past regulatory violations, ownership structures, and custody arrangements predict future fraud with an \auc{} in the range of 0.65--0.70.
\end{block}
\bottomnote{Source: Section 4.1}
\end{frame}

\begin{frame}{Question 2: RUSBoost Performance}
What AUC does Bao's RUSBoost achieve?

\vspace{0.5cm}
\begin{enumerate}[a)]
\item 0.625
\item 0.725
\item 0.825
\item 0.925
\end{enumerate}

\vspace{0.5cm}
\pause
\begin{block}{Answer}
\textbf{b) 0.725}

Bao et al. applied a RUSBoost ensemble (combining random undersampling with AdaBoost) to detect accounting fraud, achieving an \auc{} of 0.725, substantially outperforming the logistic regression baseline.
\end{block}
\bottomnote{Source: Section 4.2}
\end{frame}

\begin{frame}{Question 3: FinBERT Accuracy}
What accuracy does FinBERT achieve on financial sentiment?

\vspace{0.5cm}
\begin{enumerate}[a)]
\item 72\%
\item 79\%
\item 87\%
\item 94\%
\end{enumerate}

\vspace{0.5cm}
\pause
\begin{block}{Answer}
\textbf{c) 87\%}

FinBERT, a BERT model fine-tuned on a large corpus of financial news and communications, achieved 87\% accuracy on financial sentiment classification tasks, substantially outperforming general-purpose models.
\end{block}
\bottomnote{Source: Section 4.4}
\end{frame}

\begin{frame}{Question 4: Semi-Supervised GNN}
What AUC does Wang's semi-supervised GNN achieve?

\vspace{0.5cm}
\begin{enumerate}[a)]
\item 0.77
\item 0.82
\item 0.87
\item 0.92
\end{enumerate}

\vspace{0.5cm}
\pause
\begin{block}{Answer}
\textbf{c) 0.87}

Wang et al. applied a semi-supervised graph neural network to transaction networks, achieving an \auc{} of 0.87 and demonstrating that graph-based features substantially outperform tabular features alone.
\end{block}
\bottomnote{Source: Section 4.5}
\end{frame}

\begin{frame}{Question 5: Multi-Modal Fusion Gain}
How much AUC improvement does multi-modal fusion add?

\vspace{0.5cm}
\begin{enumerate}[a)]
\item 1--2\%
\item 3--5\%
\item 7--10\%
\item 12--15\%
\end{enumerate}

\vspace{0.5cm}
\pause
\begin{block}{Answer}
\textbf{b) 3--5\%}

Multi-modal fusion of NLP features with quantitative return data has been shown to improve detection over either modality alone, with reported improvements of approximately 3--5\% in \auc{}.
\end{block}
\bottomnote{Source: Section 4.4}
\end{frame}

\begin{frame}{Question 6: Reproducibility Crisis}
What is the main cause of the reproducibility crisis?

\vspace{0.5cm}
\begin{enumerate}[a)]
\item Lack of interest
\item Proprietary datasets
\item Insufficient computing
\item Poor documentation
\end{enumerate}

\vspace{0.5cm}
\pause
\begin{block}{Answer}
\textbf{b) Proprietary datasets}

The majority of studies use proprietary datasets---licensed commercial databases, internal regulatory records, or bespoke compilations---that are unavailable to other researchers, preventing independent verification of results.
\end{block}
\bottomnote{Source: Section 4.8}
\end{frame}

\end{document}

% ==================== SECTION 2: BACKGROUND ====================
\section{Background: The Hedge Fund Fraud Landscape}
\label{sec:background}

Hedge funds occupy a distinctive position in the financial ecosystem: they manage
trillions of dollars in assets, yet operate under substantially lighter disclosure
obligations than mutual funds or public companies. This opacity, combined with
complex trading strategies and performance-linked compensation structures, creates
fertile ground for fraudulent conduct. Before examining how artificial intelligence
can detect such fraud (\Cref{sec:pipeline,sec:literature}), we must first
understand what forms hedge fund fraud takes, which data sources are available for
detection, and what regulatory structures govern both the funds and the emerging
AI tools applied to their oversight.

% ---------- 2.1 Taxonomy ----------
\subsection{Taxonomy of Hedge Fund Fraud}
\label{sec:taxonomy}

We organize hedge fund fraud into five categories, ordered roughly from the most
frequently studied in the academic literature to the most difficult to detect with
currently available data. For each category we provide a definition, a detection
difficulty rating on a five-point scale, at least one notable enforcement case,
and the observable signals that machine learning models can exploit.
\Cref{tab:fraud-taxonomy} summarizes the taxonomy.

\begin{table}[t]
\centering
\caption{Taxonomy of hedge fund fraud types with detection difficulty ratings
and principal observable signals. Difficulty is rated on a scale from 1
(straightforward with available data) to 5 (requires privileged real-time data
and advanced methods).}
\label{tab:fraud-taxonomy}
\small
\begin{tabularx}{\textwidth}{lccX}
\toprule
\textbf{Fraud Type} & \textbf{Difficulty} & \textbf{Key Case} & \textbf{Observable Signals} \\
\midrule
Performance fabrication & 3/5 & Madoff (2008) & Serial correlation, Benford violations, implausible Sharpe ratios \\
Allocation fraud & 4/5 & Petters (2008) & Cross-account return dispersion, win-rate asymmetry \\
Strategy misrepresentation & 3/5 & Platinum Partners (2016) & Style drift, factor exposure shifts, textual inconsistencies \\
Market manipulation & 5/5 & SAC Capital (2013) & Order-flow anomalies, network centrality, timing patterns \\
Regulatory fraud & 2/5 & Lancer Mgmt.\ (2003) & Filing inconsistencies, text anomalies, omission detection \\
\bottomrule
\end{tabularx}
\end{table}

% ----- 2.1.1 Performance Fabrication -----
\subsubsection{Performance Fabrication}
\label{sec:fraud-performance}

Performance fabrication encompasses any deliberate misstatement of investment
returns, ranging from outright Ponzi schemes to subtler forms of return smoothing
and net asset value (NAV) manipulation. In a Ponzi scheme, new investor capital is
used to pay purported returns to existing investors, requiring no actual
profitable trading. Return smoothing involves selectively deferring the
recognition of losses or spreading gains across periods to produce an
artificially stable return series. NAV manipulation inflates the reported value of
illiquid positions---such as over-the-counter derivatives, distressed debt, or
private placements---for which no reliable market price exists.

The paradigmatic case remains Bernard L.\ Madoff Investment Securities, which
sustained fabricated returns for at least two decades before collapsing in
December~2008 with estimated losses of \$65~billion
\citep{gregoriou2009madoff}. Madoff's reported track record exhibited only seven
losing months across a 14-year span, producing a nearly perfect 45-degree equity
curve---a statistical near-impossibility for any legitimate trading strategy.
\citet{markopolos2010noone} famously documented these anomalies years before the
scheme was uncovered, noting that the reported Sharpe ratio exceeded plausible
bounds for the purported split-strike conversion strategy.

\textit{Detection difficulty: 3/5.} Statistical red flags for fabricated returns
are well established in the literature. \citet{bollen2012suspicious} identified
several distributional anomalies---including discontinuities at zero in the return
distribution, abnormally low serial correlation of losses, and suspiciously
consistent positive returns---that collectively flag roughly 8\% of funds in the
Lipper TASS database as potentially suspicious. \citet{getmansky2004econometric}
developed an econometric model demonstrating that serial correlation in hedge fund
returns often arises from the managed pricing of illiquid assets, providing a
quantitative baseline against which smoothed returns can be measured. Benford's
law, which predicts the frequency distribution of leading digits in naturally
occurring numerical data \citep{benford1938law,nigrini2012benford}, has also been
applied to hedge fund returns: deviations from the expected distribution can
signal data fabrication, though the test has limited power for short return
histories. While these signals are individually noisy, their combination through
machine learning classifiers offers substantially improved discriminatory
power---a theme we develop in \Cref{sec:literature}.

% ----- 2.1.2 Allocation Fraud -----
\subsubsection{Allocation Fraud}
\label{sec:fraud-allocation}

Allocation fraud occurs when a fund manager systematically directs profitable
trades to favored accounts---typically proprietary or co-investment vehicles---while
routing losing trades to client accounts. A related variant, cherry-picking,
involves delaying the allocation of executed trades until after the daily profit
or loss is known, at which point winning trades are assigned to preferred accounts.
Securities and Exchange Commission (SEC) enforcement data reveal cases in which
favored accounts received 91\% profitable trade allocations compared with only
31\% for general client accounts, a disparity that cannot arise by chance.

\textit{Detection difficulty: 4/5.} Allocation fraud is inherently difficult to
detect because it requires trade-level data---specifically, the timestamps of order
execution and the subsequent assignment of fills to accounts. Such data are
rarely available in public databases. Hedge fund return databases (discussed in
\Cref{sec:data-returns}) report only fund-level monthly returns, which obscure
intra-fund allocation patterns entirely. Even with account-level return data,
detection requires statistical comparison of return distributions across accounts
managed by the same adviser, making cross-account dispersion analysis and
win-rate asymmetry the primary observable signals. Network-based approaches that
map adviser--account relationships from regulatory filings show promise but remain
largely unexplored.

% ----- 2.1.3 Strategy Misrepresentation -----
\subsubsection{Strategy Misrepresentation}
\label{sec:fraud-strategy}

Strategy misrepresentation arises when a fund's actual investment behavior
diverges materially from its stated strategy without adequate disclosure. This
category includes undisclosed style drift---where a fund marketed as
equity-long/short gradually concentrates into illiquid credit positions---as well
as leverage misreporting and, increasingly, ``AI-washing,'' the practice of
falsely claiming that investment decisions are driven by artificial intelligence
or machine learning models when they are not.

\citet{patton2015change} developed change-point detection methods to identify
structural breaks in hedge fund risk exposures, showing that style drift often
precedes fund failure. Quantitative style analysis using rolling-window
regressions against factor benchmarks \citep{fung2001risk} can detect drift, but
it cannot distinguish between legitimate strategy evolution and fraudulent
misrepresentation without reference to the fund's disclosure documents. Natural
language processing (NLP) applied to Form ADV brochures, offering memoranda, and
investor letters can bridge this gap by comparing stated strategy descriptions
against quantitative factor exposures.

\textit{Detection difficulty: 3/5.} The combination of quantitative style analysis
and NLP on regulatory filings makes this category amenable to AI-based detection.
The principal challenge lies in establishing an appropriate threshold: some degree
of style drift is normal and even desirable in dynamic markets, and distinguishing
intentional misrepresentation from adaptive portfolio management requires
contextual judgment.

% ----- 2.1.4 Market Manipulation -----
\subsubsection{Market Manipulation}
\label{sec:fraud-manipulation}

Market manipulation by hedge funds encompasses front-running of client orders,
insider trading (including ``shadow trading'' on economically related securities
to avoid detection), and spoofing---the placement and rapid cancellation of large
orders to create a false impression of supply or demand. The 2013 guilty plea by
SAC Capital Advisors to insider trading charges, resulting in a \$1.8~billion
penalty, illustrated the scale at which hedge funds can engage in information-based
manipulation \citep{lewis2012new}.

\textit{Detection difficulty: 5/5.} Market manipulation is the most difficult
fraud category to detect because it requires real-time, tick-level trade and
order-book data, which are not available in standard hedge fund databases.
Detection methods must incorporate network analysis to identify communication
patterns between traders and information sources, temporal analysis of order
placement relative to material nonpublic events, and cross-market surveillance to
detect shadow trading across related securities. Regulators have begun deploying
such systems---the SEC's Market Information Data Analytics System (MIDAS), for
instance, ingests and analyzes billions of order and trade records daily---but
academic research in this area remains constrained by data access limitations.

% ----- 2.1.5 Regulatory Fraud -----
\subsubsection{Regulatory Fraud}
\label{sec:fraud-regulatory}

Regulatory fraud involves the submission of materially false or misleading
information in mandatory filings. For hedge fund advisers, the primary filings
include Form ADV (the uniform registration document required under the Investment
Advisers Act of 1940), Form D (Regulation D offering notices), and Form 13F
(quarterly holdings reports for institutional investment managers). Fraud in this
category ranges from deliberate misstatements---such as understating assets under
management (AUM) to avoid registration thresholds---to material omissions, such as
failing to disclose disciplinary history or conflicts of interest.

\citet{dimmock2012predicting} demonstrated that information in Form ADV filings
has predictive power for subsequent SEC enforcement actions, with prior regulatory
sanctions, ownership structures, and custody arrangements serving as significant
predictors. \citet{brown2008mandatory} examined the incremental information value
of mandatory hedge fund disclosure, finding that filing data contain
fraud-relevant signals that complement return-based statistical tests.

\textit{Detection difficulty: 2/5.} Regulatory fraud is the most tractable
category for AI-based detection because the underlying data are structured (or
semi-structured), publicly accessible through the SEC's Electronic Data
Gathering, Analysis, and Retrieval (EDGAR) system, and amenable to both
traditional text analysis and modern NLP. Cross-referencing filings with external
data---for example, verifying reported AUM against fund flows implied by return
data---can reveal inconsistencies with high precision.


% ---------- 2.2 Data Ecosystem ----------
\subsection{The Data Ecosystem for Hedge Fund Fraud Detection}
\label{sec:data-ecosystem}

The effectiveness of any fraud detection system is bounded by the quality,
coverage, and granularity of its input data. We organize the data landscape into
four layers: return data, regulatory filings, alternative data, and synthetic
data. Each layer presents distinct opportunities and challenges for AI-based
detection.

% ----- 2.2.1 Return Data -----
\subsubsection{Return Data}
\label{sec:data-returns}

The primary sources of hedge fund return data are commercial databases maintained
by Lipper TASS (now Refinitiv), Hedge Fund Research (HFR), BarclayHedge, and
Morningstar. The Lipper TASS database, the most widely used in academic research,
contains monthly return series for over 7{,}000 live and defunct funds, along with
self-reported fund characteristics such as strategy classification, inception
date, management and incentive fees, and lockup provisions.

These databases suffer from three well-documented biases that directly affect
fraud detection research. \textit{Survivorship bias} arises because funds that
cease reporting---due to liquidation, poor performance, or regulatory action---are
removed from the live database. \citet{fung2009measurement} estimated that
survivorship bias overstates average hedge fund returns by approximately 242~basis
points per year. \textit{Backfill bias} (or instant history bias) occurs when a
fund that begins reporting to a database retroactively submits its prior return
history, which tends to be favorably selected; \citet{fung2009measurement}
estimated this bias at 442~basis points per year. \textit{Selection bias} stems
from the voluntary nature of hedge fund reporting: funds with strong track records
are more likely to report (for marketing purposes), while distressed or
fraudulent funds may stop reporting before detection.

For fraud detection specifically, these biases create a pernicious asymmetry:
fraudulent funds that are eventually detected and shut down exit the database,
while fraudulent funds that evade detection continue to report. Models trained
on survivorship-biased data may therefore underestimate the base rate of fraud
and miss the statistical signatures of currently active fraudulent schemes.
Researchers must take care to use databases that include ``graveyard'' (defunct)
funds and to model the reporting decision process itself as a potential signal
\citep{agarwal2011hedge,aragon2007share}.

% ----- 2.2.2 Regulatory Filings -----
\subsubsection{Regulatory Filings}
\label{sec:data-filings}

Since the passage of the Dodd-Frank Wall Street Reform and Consumer Protection
Act in 2010, investment advisers managing \$150~million or more in AUM have been
required to register with the SEC and file Form ADV. This mandate substantially
expanded the universe of hedge funds subject to regulatory disclosure, creating a
rich data source that did not exist prior to 2012 for most hedge fund advisers.

The SEC's EDGAR system provides public access to Form ADV (Parts 1 and 2), Form D
notices, and Form 13F quarterly holdings. Form ADV Part~1 contains structured
data on the adviser's business, ownership, clients, and disciplinary history.
Part~2 (the ``brochure'') is a narrative document describing the adviser's
strategies, fee structures, risk factors, and conflicts of interest. Form 13F
provides quarterly snapshots of equity holdings for managers with more than
\$100~million in qualifying securities, enabling holdings-based style analysis.

Parsing these filings presents significant practical challenges. While Form ADV
Part~1 is filed electronically in a structured XML format through the Investment
Adviser Registration Depository (IARD), Part~2 is a free-text PDF with no
standardized structure. Form 13F data, though machine-readable, contain known
errors and inconsistencies in security identification. Merging data across filing
types and linking to commercial return databases (e.g., matching TASS fund
records to SEC adviser records) requires substantial data engineering effort
\citep{brown2008mandatory,dimmock2012predicting}. Despite these obstacles, the
combination of return data with regulatory filings yields a materially enriched
dataset: return-based statistical flags can be cross-validated against
filing-derived signals such as auditor changes, custody arrangements, and
disciplinary histories.

% ----- 2.2.3 Alternative Data -----
\subsubsection{Alternative Data}
\label{sec:data-alternative}

The rapid growth of alternative data---non-traditional data sources processed
through computational methods---has opened new avenues for fraud detection that
extend beyond the return and filing data traditionally available to researchers and
regulators. The global alternative data market was estimated at approximately
\$7.5~billion in 2023, with projections reaching \$273~billion by 2032, driven
primarily by adoption among investment managers and financial regulators.

Relevant alternative data sources for hedge fund fraud detection include news and
social media sentiment, which can flag reputational signals and emerging concerns
about specific funds or managers before they appear in regulatory actions.
Satellite imagery and geolocation data have been used to independently verify
economic claims---for instance, estimating retail foot traffic or commodity
inventory levels to cross-check reported fund performance. Web traffic analytics,
patent filings, and litigation records provide additional dimensions for
triangulating the plausibility of stated strategies and returns.

However, alternative data carry their own risks. Sentiment-derived signals are
noisy and susceptible to manipulation through coordinated social media campaigns.
The costs of acquiring and processing satellite and geolocation data are
substantial, and the relationship between these data and fund-level fraud signals
is often indirect. Moreover, the use of alternative data for surveillance raises
privacy concerns that intersect with emerging regulatory frameworks, particularly
the European Union's AI Act (discussed in \Cref{sec:regulatory-eu}).
Nevertheless, for regulators and fund-of-funds managers with access to these data,
they represent a valuable complementary layer that AI systems can integrate through
multi-modal learning architectures.

% ----- 2.2.4 Synthetic Data -----
\subsubsection{Synthetic Data}
\label{sec:data-synthetic}

A fundamental obstacle in hedge fund fraud detection is the extreme class
imbalance inherent in the problem: confirmed fraud cases represent a small
fraction of the total fund population, making it difficult to train supervised
classifiers with adequate positive-class representation. Synthetic data
generation techniques address this imbalance by creating artificial examples of
fraudulent fund behavior.

The Synthetic Minority Over-sampling Technique \citep[SMOTE;][]{chawla2002smote}
remains the most widely used approach, generating synthetic minority-class
samples by interpolating between existing positive examples in feature space.
More recent methods employ generative adversarial networks (GANs) and
variational autoencoders (VAEs) to produce synthetic return series and feature
vectors that capture the distributional properties of known fraudulent funds.
These generative approaches can produce more realistic and diverse synthetic
samples than interpolation-based methods, but they introduce their own
validation challenges: synthetic data must preserve the statistical dependencies
and temporal dynamics of real fraud cases without amplifying artifacts of the
training data.

Privacy-preserving synthetic data generation is an active area of research with
particular relevance for cross-institutional collaboration. Regulators who possess
enforcement-labelled datasets cannot share them with academic researchers due to
confidentiality constraints. Differentially private generative models could
enable the release of synthetic fraud datasets that preserve aggregate statistical
properties while protecting the identities of individual funds and managers.
Although this approach remains largely aspirational in the hedge fund domain, it
has shown promise in adjacent areas of financial crime detection, including
anti-money laundering and credit fraud.

% ---------- 2.3 Regulatory Context ----------
\subsection{Regulatory Context}
\label{sec:regulatory}

The regulatory environment shapes both the data available for fraud detection and
the constraints under which AI-based detection systems must operate. We review the
principal frameworks in the United States and European Union, followed by the
emerging field of supervisory technology (SupTech).

% ----- 2.3.1 US Regulatory Framework -----
\subsubsection{United States}
\label{sec:regulatory-us}

The US regulatory framework for hedge fund oversight underwent a structural
transformation following the 2008 financial crisis and the Madoff scandal.
Title~IV of the Dodd-Frank Act eliminated the ``private adviser exemption'' that
had previously allowed most hedge fund advisers to avoid SEC registration,
requiring advisers with AUM of \$150~million or more to register and file
Form~ADV. This single regulatory change dramatically expanded the universe of
funds subject to systematic oversight and created the filing data that now
underpin many detection approaches \citep{brown2008mandatory}.

The SEC has invested substantially in computational enforcement capabilities.
The Division of Economic and Risk Analysis (DERA), established in 2009,
provides quantitative analysis to support enforcement investigations and
rulemaking. MIDAS, operational since 2013, collects and analyzes data from all
equity exchanges and off-exchange venues, processing approximately one billion
records per day to detect market manipulation patterns. The Center for Risk and
Quantitative Analytics (CRQA), housed within the Office of Compliance Inspections
and Examinations, maintains databases of trading patterns derived from past
enforcement actions and uses these to prioritize future examinations
\citep{sec2023priorities}.

The SEC's Whistleblower Program, established under Dodd-Frank Section~922 in
response to the failure to act on \citeauthor{markopolos2010noone}'s
(\citeyear{markopolos2010noone}) repeated warnings about Madoff, has become a
significant source of enforcement leads. Since its inception, the program has
awarded over \$1.5~billion to whistleblowers and generated thousands of tips
that complement algorithmic detection. The interaction between human intelligence
(whistleblower tips) and machine intelligence (quantitative screening) represents
an underexplored hybrid detection paradigm.

% ----- 2.3.2 EU Regulatory Framework -----
\subsubsection{European Union}
\label{sec:regulatory-eu}

In the European Union, the Alternative Investment Fund Managers Directive
(AIFMD), adopted in 2011, established a harmonized regulatory framework for
managers of alternative investment funds, including hedge funds. AIFMD imposes
reporting obligations, leverage limits, and investor disclosure requirements
that generate structured data analogous---though not identical---to the SEC's
Form~ADV regime.

More directly relevant to AI-based fraud detection is the EU Artificial
Intelligence Act (Regulation 2024/1689), which entered into force in August~2024
\citep{eu2024aiact}. The AI Act classifies AI systems used for ``creditworthiness
assessment'' and ``fraud detection in financial services'' as high-risk,
subjecting them to mandatory requirements including risk management systems,
data governance standards, technical documentation, human oversight provisions,
and transparency obligations. Notably, the Act requires that high-risk AI
systems provide outputs that are ``sufficiently transparent to enable deployers
to interpret the system's output and use it appropriately'' (Art.~13), which
has direct implications for model selection in fraud detection: opaque models
such as deep neural networks may require post-hoc explainability methods
(e.g., SHAP, LIME) to satisfy regulatory requirements, while inherently
interpretable models such as logistic regression or decision trees may be
preferred despite potentially lower predictive performance.

The tension between predictive accuracy and explainability is not merely
academic. A detection system that identifies suspicious funds but cannot
articulate the basis for its suspicion is of limited use to regulators who must
justify enforcement actions in administrative proceedings and courts. The
EU AI Act codifies this intuition into law, and similar requirements are likely
to emerge in other jurisdictions. We return to this tension in
\Cref{sec:research-agenda}.

% ----- 2.3.3 SupTech Adoption -----
\subsubsection{Supervisory Technology}
\label{sec:regulatory-suptech}

Supervisory technology (SupTech) refers to the use of advanced analytics and AI
by financial regulators and central banks to enhance their oversight capabilities
\citep{fsb2017ai,bis2024suptech}. The adoption of SupTech for hedge fund oversight
represents a shift from reactive enforcement---investigating fraud after
losses have materialized---to proactive surveillance that aims to detect anomalies
before they escalate.

Several regulatory agencies have reported SupTech initiatives relevant to hedge
fund fraud detection. The SEC's DERA and CRQA units, discussed above, represent
early examples. The Bank of England, the Monetary Authority of Singapore, and the
De Nederlandsche Bank have all piloted machine learning systems for anomaly
detection in financial reporting data. These systems typically operate by
establishing baseline behavioral profiles for regulated entities and flagging
deviations that exceed statistical thresholds, an approach conceptually similar
to the return-based statistical tests pioneered by
\citet{bollen2012suspicious} but applied at institutional scale across multiple
firms simultaneously.

Despite these advances, SupTech adoption faces significant challenges. Regulators
must manage the risk of false positives, which consume scarce examination
resources, while simultaneously ensuring that sophisticated fraudsters cannot
reverse-engineer detection criteria. The shortage of personnel with combined
expertise in financial regulation and machine learning constrains implementation.
Cross-border coordination remains limited: a fund registered in the Cayman
Islands, managed from New York, with European investors and Asian trading venues
may fall within the jurisdiction of multiple regulators, none of whom possess a
complete picture. AI-based detection systems that operate on fragmented,
jurisdiction-specific data inevitably produce a partial view of potentially
global fraudulent schemes.

\bigskip
\noindent
This section has established the conceptual and empirical foundations for the
remainder of the paper. The fraud taxonomy (\Cref{sec:taxonomy}) identifies what
must be detected; the data ecosystem (\Cref{sec:data-ecosystem}) characterizes
the inputs available to detection systems; and the regulatory context
(\Cref{sec:regulatory}) defines the institutional constraints under which these
systems operate. \Cref{sec:pipeline} builds on this foundation by specifying the
technical architecture of an end-to-end AI-based detection pipeline.

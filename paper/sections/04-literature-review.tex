% ==================== SECTION 4: LITERATURE REVIEW ====================
% NOTE: This is a qualitative literature review, NOT a meta-analysis
\section{Review of AI-Based Detection Methods}
\label{sec:literature}

This section reviews the literature on AI and machine learning methods applied to hedge fund fraud detection and adjacent financial fraud domains. We organize the review around method families rather than chronologically, enabling direct comparison of approaches and systematic identification of gaps. For each family, we discuss the key studies, reported effectiveness, and limitations specific to the hedge fund context. Where direct evidence from hedge fund applications is unavailable, we draw on results from related financial fraud domains---accounting fraud, credit card fraud, and anti-money laundering---while carefully noting the assumptions required for transferability. The fraud taxonomy and data ecosystem established in \Cref{sec:background} provide the evaluative lens: a method is assessed not only on its reported accuracy but on its applicability to the specific fraud types, data modalities, and operational constraints that characterize the hedge fund industry.


% ---------- 4.1 Classical Statistical and Rule-Based Approaches ----------
\subsection{Classical Statistical and Rule-Based Approaches}
\label{sec:lit-classical}

The earliest detection methods for hedge fund fraud are rooted in forensic statistics and rule-based auditing procedures. These approaches predate the machine learning era but remain foundational both as standalone screening tools and as feature generators for more complex models.

Benford's law, which predicts that the leading digit $d$ in naturally occurring numerical data follows the distribution $P(d) = \log_{10}(1 + 1/d)$ \citep{benford1938law}, has been extensively applied to financial data forensics. \citet{nigrini2012benford} systematized its use for fraud detection, demonstrating that fabricated numerical data---because humans are poor intuitive generators of logarithmic digit distributions---tend to exhibit statistically significant deviations from Benford's expected frequencies. Applied retrospectively to Madoff's reported returns, automated digit-frequency tests flagged anomalies in nine out of ten statistical tests, lending credibility to the approach as a screening tool. However, Benford's law requires sufficiently large samples to achieve adequate statistical power, a condition that is rarely satisfied by individual hedge fund return series comprising at most 120--240 monthly observations. Moreover, a knowledgeable fraudster---aware that regulators employ Benford tests---can engineer return series that satisfy digit-frequency constraints while remaining fabricated in substance.

Serial correlation analysis represents a second classical pillar. \citet{getmansky2004econometric} developed a moving average model, $\text{MA}(k)$, for hedge fund returns that decomposes observed returns into a true economic component and a smoothing component attributable to the managed pricing of illiquid assets. Their analysis demonstrated that 30--40\% of hedge funds in the Lipper TASS database exhibit statistically significant positive serial correlation at lag one, consistent with return smoothing. While serial correlation alone does not imply fraud---illiquidity in legitimate portfolios produces similar signatures---the magnitude and persistence of serial correlation, particularly when inconsistent with a fund's stated strategy and asset class, constitutes a valuable fraud indicator. The contribution of this work to subsequent ML-based approaches is primarily methodological: the smoothing parameters estimated under the $\text{MA}(k)$ model are now standard features in supervised classification systems.

\citet{bollen2012suspicious} advanced distributional analysis by identifying a ``kink'' at zero in the return distributions of a subset of hedge funds---a discontinuity in which small positive returns appear with significantly higher frequency than small negative returns. This pattern, interpreted as evidence that managers selectively defer or reclassify marginal losses, correctly identified approximately 50\% of funds that subsequently faced SEC enforcement actions. \citet{brown2008mandatory} developed an operational risk scoring approach, the omega-score, derived from Form~ADV data that captures governance and organizational risk factors; funds scoring above the threshold experienced significantly higher failure and regulatory action rates. \citet{dimmock2012predicting} applied logistic regression to SEC filing data, demonstrating that past regulatory violations, ownership structures, and custody arrangements predict future fraud with an \auc{} in the range of 0.65--0.70.

The collective limitation of these classical approaches is their specialization: each detects a specific statistical signature of a specific fraud type. Benford tests identify digit manipulation; serial correlation analysis detects return smoothing; distributional discontinuity analysis flags selective loss avoidance. None captures the multi-dimensional, heterogeneous patterns that characterize sophisticated fraud, and all exhibit high false positive rates when deployed independently. Their enduring value lies in their complementarity: the features they generate---serial correlation coefficients, Benford test statistics, distributional shape parameters, operational risk scores---form the bedrock of the feature engineering stage in modern ML pipelines (see \Cref{sec:pipeline}).


% ---------- 4.2 Tree-Based and Ensemble Methods ----------
\subsection{Tree-Based and Ensemble Methods}
\label{sec:lit-trees}

Tree-based ensemble methods currently dominate tabular fraud detection across the financial industry, and their strengths align particularly well with the hedge fund context. Random forests \citep{breiman2001random} aggregate predictions from hundreds of independently grown decision trees, each trained on a bootstrap sample with random feature subsets, yielding models that are robust to overfitting, tolerant of missing data, and capable of handling mixed feature types---numerical return statistics alongside categorical variables such as strategy classification and auditor identity---without requiring feature standardization.

The advent of gradient boosting machines, particularly XGBoost \citep{chen2016xgboost}, LightGBM \citep{ke2017lightgbm}, and CatBoost \citep{prokhorenkova2018catboost}, elevated ensemble performance further. These algorithms construct trees sequentially, with each tree correcting the residuals of its predecessors, achieving state-of-the-art accuracy on tabular financial data across numerous benchmarks. Their built-in mechanisms for handling class imbalance---sample weighting, stratified subsampling, and cost-sensitive learning---are directly relevant to the skewed class distributions inherent in fraud detection.

The most rigorous large-scale application of ensemble methods to financial fraud detection is the work of \citet{bao2020detecting}, who applied a RUSBoost ensemble---combining random undersampling with AdaBoost---to detect accounting fraud in publicly traded US firms. Training on over 28,000 firm-year observations linked to SEC Accounting and Auditing Enforcement Releases (AAERs), their model achieved an \auc{} of 0.725, substantially outperforming the logistic regression benchmark of \citet{dimmock2012predicting}. Although this study targeted accounting fraud rather than hedge fund fraud specifically, the methodological template---ensemble learning on SEC enforcement labels with systematic feature engineering from regulatory filings---is directly transferable. More recent work on stacking ensembles that combine XGBoost, LightGBM, and CatBoost through a meta-learner has reported $\fone$ scores approaching 0.88 in financial fraud detection tasks, the highest among individual method families in the broader fraud detection literature \citep{hilal2022financial}.

A critical strength of tree-based ensembles for the hedge fund domain is their compatibility with modern explainability methods. SHAP (SHapley Additive exPlanations) values, for which \citet{lundberg2017unified} provide exact computation algorithms for tree-based models, decompose each prediction into per-feature contributions, enabling explanations of the form: ``this fund was flagged because its serial correlation at lag one is 2.3 standard deviations above the strategy-peer median, its Sharpe ratio is implausibly high given its stated volatility, and its auditor has been associated with two previously sanctioned funds.'' Such explanations satisfy the transparency requirements discussed in \Cref{sec:adversarial} and align with regulatory expectations for evidence-based enforcement.

The principal limitation of tree-based methods in this context is their reliance on supervised training with labeled fraud data. With only 50--100 confirmed hedge fund fraud cases across the historical record of SEC enforcement, the effective training set for the positive class is perilously small. Ensemble methods can mitigate class imbalance through resampling and cost-sensitive weighting, but they cannot overcome the fundamental heterogeneity of the positive class: the statistical signature of a Ponzi scheme differs qualitatively from that of NAV manipulation or style drift, and a model trained on pooled fraud labels may learn a compromise decision boundary that fails to detect any individual fraud type with high sensitivity.


% ---------- 4.3 Deep Learning Approaches ----------
\subsection{Deep Learning Approaches}
\label{sec:lit-deep}

Deep learning architectures offer the potential to capture complex, nonlinear patterns in hedge fund data that tree-based methods may miss, but their application to this domain is constrained by data scarcity and opacity.

Long Short-Term Memory (LSTM) networks \citep{hochreiter1997long} process sequential data through gated memory cells that selectively retain or discard information across time steps, making them naturally suited to the analysis of monthly return series. LSTMs can detect temporal patterns indicative of return smoothing---gradual shifts in serial correlation structure, time-varying volatility suppression, and regime-dependent anomalies---that fixed-window statistical tests cannot capture. The sequential nature of LSTM processing aligns with the temporal dynamics of hedge fund fraud, which typically escalates over months or years rather than occurring as a single discrete event.

Convolutional neural networks (CNNs), originally developed for image recognition \citep{lecun2015deep}, have been adapted for financial time series analysis through the encoding of return sequences as two-dimensional representations---for example, mapping monthly returns to Gramian Angular Fields or recurrence plots---that CNN architectures can process using spatial pattern recognition. Hybrid CNN-LSTM architectures that apply convolutional layers for local feature extraction followed by recurrent layers for temporal aggregation have shown promise for multi-scale temporal analysis, capturing both short-term return anomalies and longer-term behavioral shifts.

Transformer architectures \citep{vaswani2017attention}, which employ self-attention mechanisms to model dependencies across arbitrary positions in a sequence, have emerged as a powerful alternative to recurrent models for sequential data. In the hedge fund context, transformers offer a specific advantage: their attention mechanism can capture long-range dependencies across sparse monthly return series---linking, for example, suspicious return patterns in a fund's first year of operation to anomalies that emerge three years later---without the vanishing gradient problems that limit LSTM effectiveness on long sequences.

Autoencoders represent the most directly applicable deep learning paradigm for hedge fund fraud detection under conditions of label scarcity. Trained to reconstruct input data through a compressed latent representation, autoencoders learn the distributional properties of normal fund behavior; funds with high reconstruction error---whose return characteristics deviate significantly from the learned normal distribution---are flagged as anomalous. Deep autoencoders have achieved an \auc{} of approximately 0.79 on hedge fund return data in anomaly detection frameworks \citep{chalapathy2019deep}, competitive with supervised approaches despite requiring no fraud labels. This unsupervised formulation sidesteps the labeled data bottleneck and avoids the data poisoning vulnerability that afflicts supervised methods (see \Cref{sec:adversarial-threat}).

The limitations of deep learning in this domain are substantial. Neural networks are data-hungry: modern architectures typically require thousands to millions of training examples to learn effective representations, yet the hedge fund universe comprises roughly 10,000--15,000 funds with at most 120--240 monthly return observations each. Overfitting is a pervasive risk, particularly for models with millions of parameters trained on datasets with fewer than 100 positive examples. Deep models are also opaque, producing predictions that resist human interpretation---a liability under the explainability requirements of the EU AI Act discussed in \Cref{sec:regulatory-explainability}. Hyperparameter sensitivity compounds these concerns: small changes in architecture, learning rate, or regularization can produce qualitatively different results, undermining reproducibility and the reliability of reported performance claims.


% ---------- 4.4 Natural Language Processing for Financial Filings ----------
\subsection{Natural Language Processing for Financial Filings}
\label{sec:lit-nlp}

The text content of regulatory filings, offering memoranda, and investor communications constitutes a rich but underexploited data modality for hedge fund fraud detection. The evolution from bag-of-words representations to domain-specific transformer models has dramatically expanded the information that can be extracted from financial text.

Early NLP approaches to financial fraud relied on word-frequency features and domain-specific sentiment dictionaries. \citet{loughran2011liability} demonstrated that general-purpose sentiment lexicons---such as the Harvard General Inquirer---perform poorly on financial text because many words with negative general sentiment (e.g., ``liability,'' ``tax,'' ``depreciation'') carry neutral or technical meaning in financial contexts. Their financial-domain sentiment dictionary, refined in subsequent work \citep{loughran2016textual}, provided more accurate sentiment classification for SEC filings and established the principle that financial NLP requires domain-adapted lexical resources.

The transformer revolution in NLP has produced models specifically adapted to financial language. FinBERT \citep{araci2019finbert}, a BERT model \citep{devlin2019bert} fine-tuned on a large corpus of financial news and communications, achieved 87\% accuracy on financial sentiment classification tasks, substantially outperforming general-purpose models. SEC-BERT \citep{loukas2022secbert}, pre-trained specifically on SEC EDGAR filings, improved named entity recognition and document classification for regulatory texts by learning the distinctive vocabulary, syntactic patterns, and discourse structures of financial filings.

Applications to hedge fund fraud detection exploit several textual signals. First, vague or evasive language in strategy descriptions---excessive use of hedging phrases, abstract terminology, or circular definitions---may indicate an attempt to obscure the true nature of a fund's investment activities. Second, changes in filing complexity over time, measured through readability indices, sentence length distributions, and lexical diversity, have been associated with subsequent regulatory action: funds that progressively increase the opacity of their disclosures may be concealing deteriorating performance or emerging fraud. Third, boilerplate deviation analysis identifies funds whose language departs from the standard templates used by legitimate funds; while modest deviation reflects genuine strategic differentiation, extreme deviation---or suspiciously precise conformity---may signal either carelessness or deliberate obfuscation.

Multi-modal fusion of NLP features with quantitative return data has been shown to improve detection over either modality alone, with reported improvements of approximately 3--5\% in \auc{} \citep{ahmed2024survey}. The logic is intuitive: a fund whose textual descriptions claim a conservative equity long/short strategy but whose return statistics exhibit factor exposures consistent with leveraged distressed credit presents a stronger fraud signal than either the textual or quantitative anomaly alone.

The limitations of NLP-based detection are both practical and structural. Regulatory filings are submitted quarterly or annually, introducing substantial detection lag. Filings are heavily boilerplate, with much of the text carrying minimal informational content---a low signal-to-noise ratio that challenges even sophisticated language models. Most critically, the standardized nature of financial filings means that fraudulent managers can---and do---match the expected linguistic patterns precisely, using compliance counsel and template providers to produce documents that pass textual screening while concealing substantive fraud. NLP is therefore most valuable as a complementary modality within a multi-modal detection framework rather than as a standalone screening tool.


% ---------- 4.5 Graph Neural Networks for Fund Networks ----------
\subsection{Graph Neural Networks for Fund Networks}
\label{sec:lit-gnn}

Graph neural networks (GNNs) represent a fundamentally different approach to fraud detection, operating on relational structures rather than individual entity features. In the hedge fund context, the relevant graph is the network of relationships among funds, fund managers, auditors, administrators, custodians, prime brokers, and investors---a heterogeneous, temporal graph whose topology carries fraud-relevant information that is invisible to methods operating on tabular data.

The foundational work of \citet{kipf2017semi} on graph convolutional networks (GCNs) established that node classification in graphs can be performed by aggregating features from neighboring nodes through learned convolutional filters. \citet{hamilton2017inductive} introduced GraphSAGE, an inductive learning framework that generates embeddings for unseen nodes by sampling and aggregating features from a node's local neighborhood---a capability critical for scoring newly launched hedge funds that did not exist during model training (the cold-start problem discussed in \Cref{sec:op5}). Graph attention networks (GATs), introduced by \citet{velickovic2018graph}, apply attention mechanisms to weight the contributions of different neighbors, enabling the model to prioritize the most informative relationships.

The application of GNNs to financial fraud detection has yielded promising results. \citet{wang2019semi} applied a semi-supervised GNN to transaction networks, achieving an \auc{} of 0.87 and demonstrating that graph-based features substantially outperform tabular features alone for network-embedded fraud. \citet{liu2021pick} addressed the ``camouflage'' problem---in which fraudulent actors strategically surround themselves with legitimate connections to evade graph-based detection---using a GAT architecture with heterogeneous neighbor sampling that identifies fraud even when the immediate neighborhood appears benign.

The application of these methods to hedge fund networks is theoretically compelling but empirically underexplored. A hedge fund's service provider network---its auditor, administrator, custodian, and prime broker---constitutes a governance ecosystem whose composition correlates with fraud risk. Funds that employ small, unregistered auditors, lack independent administrators, or concentrate their service provider relationships within a small cluster of interconnected entities exhibit elevated fraud risk \citep{brown2008mandatory}. Temporal knowledge graphs that track the evolution of these relationships over time can capture dynamic signals: a fund's sudden change of auditor, an administrator's simultaneous association with multiple subsequently sanctioned funds, or a manager's departure from one fund and rapid launch of another with the same service provider constellation.

The strengths of GNN-based approaches are distinctive: they capture relational and structural information---guilt by association, network centrality, and structural equivalence---that is fundamentally inaccessible to methods operating on individual fund features. The primary limitations are practical. Graph construction for the hedge fund domain requires entity resolution across multiple databases---matching SEC filing entities to commercial database records to prime brokerage relationships---a data engineering challenge that is largely unaddressed in the literature. Relationship data are often incomplete: not all service provider relationships are disclosed in public filings, and the depth of relationships (e.g., the extent of an auditor's verification procedures) is unobservable. Computational cost scales with graph size and density, and training GNNs on the full hedge fund ecosystem graph, including temporal edges, demands substantial infrastructure.


% ---------- 4.6 Semi-Supervised and Self-Supervised Methods ----------
\subsection{Semi-Supervised and Self-Supervised Methods}
\label{sec:lit-semisupervised}

The fundamental label scarcity problem in hedge fund fraud detection---fewer than 100 confirmed cases against a background of 10,000+ funds---motivates methods that can leverage the vast majority of unlabeled data. Semi-supervised and self-supervised approaches offer principled frameworks for doing so.

Label propagation and self-training algorithms extend sparse label information through the data manifold: a small number of labeled fraud and non-fraud examples propagate their labels to nearby unlabeled examples in feature space, iteratively expanding the labeled set. These methods are effective when fewer than 5\% of the data carry labels, a condition that is characteristic of the hedge fund context. Self-training, in which a classifier iteratively labels its most confident predictions and retrains on the expanded set, can achieve substantial performance gains when the initial model is reasonably well-calibrated.

Contrastive learning represents a more recent and potentially more powerful paradigm. By learning representations that maximize agreement between differently augmented views of the same fund (positive pairs) while minimizing agreement between different funds (negative pairs), contrastive methods produce embeddings that separate normal from anomalous behavior without requiring explicit fraud labels \citep{pang2021deep}. The resulting representations can then be used for downstream classification with very few labeled examples, addressing both the label scarcity and the heterogeneity of the positive class.

Self-supervised pre-training on unlabeled fund returns---using objectives such as masked return prediction (predicting held-out months from context), temporal order prediction (determining whether a return sequence is in correct chronological order), or next-period forecasting---creates general-purpose representations of fund behavior that capture distributional regularities in the hedge fund universe. These representations can subsequently be fine-tuned for fraud detection with minimal labeled data, following the pre-train-then-fine-tune paradigm that has proven successful across NLP, computer vision, and other domains. Transfer learning from related financial domains---adapting models trained on banking fraud, insurance fraud, or accounting fraud to the hedge fund context---offers an additional avenue for overcoming data limitations, though the degree of transferability across fraud domains remains an empirical question.

The strength of these methods lies in their direct engagement with the label scarcity problem. Their limitation is sensitivity to distributional assumptions: performance degrades when the unlabeled data distribution shifts over time or when the labeled examples are not representative of the broader fraud population. Careful validation protocols---including temporal out-of-sample evaluation and robustness checks under distribution shift---are essential to guard against silent failure, in which a model appears to perform well on standard metrics while systematically missing emerging fraud patterns.


% ---------- 4.7 Synthetic Data and Data Augmentation ----------
\subsection{Synthetic Data and Data Augmentation}
\label{sec:lit-synthetic}

Data augmentation techniques address the class imbalance problem by generating synthetic examples of the minority class, expanding the effective training set for supervised and semi-supervised methods.

SMOTE \citep{chawla2002smote} remains the most widely used augmentation technique in financial fraud detection. By creating synthetic minority-class samples through interpolation between existing positive examples in feature space, SMOTE increases the representation of the fraud class without duplicating existing examples. However, as discussed in the context of OP4 (\Cref{sec:op4}), SMOTE's interpolation assumption is problematic for hedge fund fraud: interpolating between a Ponzi scheme and a valuation fraud generates synthetic examples that correspond to no plausible fraud pattern. Adaptive variants such as Borderline-SMOTE and ADASYN \citep{douzas2018effective}, which concentrate synthetic sample generation near the decision boundary, partially address this issue but do not resolve the fundamental heterogeneity of the positive class.

Generative adversarial networks \citep[GANs;][]{goodfellow2014generative} offer a more sophisticated approach to synthetic data generation. Conditional GANs, which condition the generation process on fraud type or other attributes, can produce synthetic fraud examples that preserve the distributional properties of specific fraud subtypes. Because GANs learn the data distribution implicitly through adversarial training, they can capture complex dependencies---between return statistics, filing characteristics, and operational features---that interpolation-based methods discard. Variational autoencoders \citep[VAEs;][]{kingma2014auto} provide an alternative generative framework with better-calibrated uncertainty estimates, which is advantageous for fraud detection applications where the confidence of generated examples matters.

The practical application of these methods to the hedge fund domain involves augmenting the 50--100 known fraud cases to create training sets of viable size for supervised learning. However, synthetic data generation introduces a validation circularity: the generator learns to produce data that resembles the known fraud examples, but if the known examples are not representative of the full spectrum of hedge fund fraud, the synthetic data will perpetuate and amplify the biases of the original sample. Ensuring that synthetic fraud cases are realistic without assuming that we already know what fraud looks like is a fundamental methodological challenge.

Recent work has proposed synthetic benchmark datasets for fraud detection research as a means of addressing the lack of public data \citep{fiore2019using}. These benchmarks, generated through calibrated simulation rather than direct augmentation of proprietary data, can provide standardized evaluation resources while avoiding the confidentiality constraints that prevent the release of real regulatory data. The development of hedge-fund-specific synthetic benchmarks, calibrated to the empirical properties of real hedge fund returns and fraud cases, remains an open priority (see OP1 in \Cref{sec:op1}).


% ---------- 4.8 Critical Assessment of the Literature ----------
\subsection{Critical Assessment of the Literature}
\label{sec:lit-assessment}

The preceding subsections have documented a broad and growing body of work, but a candid assessment reveals significant structural weaknesses that limit the field's maturity and the reliability of its claims.

\paragraph{Reproducibility.} The majority of studies in this domain use proprietary datasets---licensed commercial databases, internal regulatory records, or bespoke compilations---that are unavailable to other researchers. Results cannot be independently verified, and reported performance metrics must be taken on trust. This contrasts sharply with adjacent fields such as credit card fraud detection, where public benchmarks have enabled rigorous, reproducible comparison across hundreds of methods and research groups.

\paragraph{Benchmark gap.} No standard benchmark dataset exists for hedge fund fraud detection. Each study assembles its own data, defines its own fraud labels, and reports results on non-overlapping fund populations. Cross-study comparison is therefore effectively impossible: a reported \auc{} of 0.79 in one study cannot be meaningfully compared to an \auc{} of 0.72 in another when the datasets, label definitions, feature sets, and evaluation protocols differ in every respect.

\paragraph{Domain specificity.} The most impressive performance claims in the literature---$\fone$ scores above 0.85, \auc{} values above 0.90---originate from adjacent domains (credit card fraud, payment fraud, banking fraud) where labeled data are abundant and the statistical properties of fraud are well characterized. The transferability of these results to the hedge fund context, with its sparse data, heterogeneous fraud types, and sophisticated adversaries, is uncertain at best. Studies that report high performance on financial fraud benchmarks may overestimate effectiveness when applied to the specific conditions of hedge fund surveillance \citep{bolton2002statistical, phua2010comprehensive}.

\paragraph{Class imbalance handling.} The treatment of class imbalance varies enormously across studies and is often inadequately reported. Some studies apply SMOTE without evaluating its impact; others use cost-sensitive learning with ad hoc cost ratios; still others report only accuracy---a metric that is meaningless under severe class imbalance, as a model that classifies all funds as non-fraudulent achieves accuracy above 97\%. The absence of standardized protocols for handling and reporting class imbalance makes performance claims across studies difficult to compare.

\paragraph{Evaluation protocols.} The use of temporal train-test splits---training on data from period $t$ and evaluating on data from period $t+1$---is inconsistent across the literature. Many studies employ random cross-validation, which allows information from the future to leak into the training set and inflates reported performance. In a domain where fraud patterns evolve over time and concept drift is a known challenge, temporal evaluation is not a methodological refinement but a necessity; its absence in a substantial fraction of studies undermines confidence in reported results.

\paragraph{Publication bias.} Positive results are preferentially published: studies reporting high detection rates are more likely to reach peer-reviewed venues than studies reporting null or modest results. The true performance landscape of AI-based hedge fund fraud detection is therefore likely less optimistic than the published literature suggests. Registered reports and pre-registered analysis plans, which commit to publication regardless of outcome, could partially address this bias but are not yet standard practice in this field.

\paragraph{Overfitting risk.} With only approximately 50--100 labeled fraud cases in the historical record, even moderate-complexity models risk overfitting to the idiosyncratic characteristics of specific fraud schemes rather than learning generalizable fraud patterns. A model that achieves high performance on a held-out sample of, say, 20 fraud cases may simply have memorized the statistical fingerprints of the specific Ponzi schemes, valuation frauds, and style misrepresentations in its training set, without acquiring the capacity to detect novel fraud types. This overfitting risk is amplified by the common practice of reporting results on a single train-test split rather than multiple independent evaluations.

\bigskip
\noindent
Taken together, these concerns suggest that the field of AI-based hedge fund fraud detection is at an early stage of scientific maturity. Substantial progress in detection capability is likely achievable, but realizing this potential requires addressing the data, evaluation, and reproducibility challenges identified above. The research agenda presented in \Cref{sec:research-agenda} proposes concrete steps toward this goal, beginning with the creation of standardized benchmarks (OP1) and the development of methods tailored to the extreme small-sample, heterogeneous-class conditions that define this domain (OP4).

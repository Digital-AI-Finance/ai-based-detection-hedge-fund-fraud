[
  {
    "question": "How many adversarial attack vectors are identified?",
    "options": ["a) 2", "b) 3", "c) 4", "d) 5"],
    "correct_answer": "c",
    "explanation": "Four principal attack vectors are identified: data poisoning, evasion attacks, model extraction, and strategic timing and regime exploitation.",
    "section_reference": "Section 5.1"
  },
  {
    "question": "What is the mean AUC degradation under adversarial attack?",
    "options": ["a) 5.3%", "b) 8.2%", "c) 10.6%", "d) 15.4%"],
    "correct_answer": "c",
    "explanation": "Recent work on adversarial robustness in financial ML reports a mean AUC degradation of 10.6% across surveyed detection systems under adversarial attack.",
    "section_reference": "Section 5.1"
  },
  {
    "question": "How much AUC does adversarial training recover?",
    "options": ["a) 30-40%", "b) 45-55%", "c) 60-70%", "d) 80-90%"],
    "correct_answer": "c",
    "explanation": "Robust optimization applied to financial fraud detection models can recover 60-70% of the AUC lost to adversarial attacks, reducing attack success rates from approximately 35% to 5%.",
    "section_reference": "Section 5.2"
  },
  {
    "question": "Which EU AI Act article addresses transparency?",
    "options": ["a) Article 9", "b) Article 13", "c) Article 14", "d) Article 52"],
    "correct_answer": "b",
    "explanation": "Article 13 of the EU AI Act mandates transparency: high-risk AI systems must be designed to enable users to interpret the system's output and use it appropriately.",
    "section_reference": "Section 5.3"
  },
  {
    "question": "Which EU AI Act article addresses human oversight?",
    "options": ["a) Article 9", "b) Article 13", "c) Article 14", "d) Article 52"],
    "correct_answer": "c",
    "explanation": "Article 14 of the EU AI Act requires human oversight: high-risk systems must allow effective oversight by natural persons, including the ability to override the system's output.",
    "section_reference": "Section 5.3"
  },
  {
    "question": "What percentage of institutions lack adversarial resilience policies?",
    "options": ["a) 45%", "b) 58%", "c) 68%", "d) 78%"],
    "correct_answer": "d",
    "explanation": "A survey of financial institutions found that 78% lacked formal adversarial resilience policies for their ML-based detection systems, suggesting a wide gap between threat landscape and preparedness.",
    "section_reference": "Section 5.1"
  },
  {
    "question": "What is the data poisoning degradation range?",
    "options": ["a) 1-3%", "b) 5-12%", "c) 15-25%", "d) 30-40%"],
    "correct_answer": "b",
    "explanation": "Data poisoning attacks can degrade model performance by 5-12% even when the fraction of poisoned samples is small, particularly concerning given class imbalance amplifies the impact of corrupted labels.",
    "section_reference": "Section 5.1"
  }
]
